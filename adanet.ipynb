{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade tensorflow\n",
    "! pip install -q grpcio==1.24.3\n",
    "! pip install -q tensorflow==2.0.0\n",
    "! pip install -q adanet==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "SEED=123\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "from sklearn.metrics import brier_score_loss, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import adanet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
    "    def input_function():\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(1000, seed=SEED)\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "        return ds\n",
    "    return input_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(dat):\n",
    "    pred_dicts = list(dat)\n",
    "    probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n",
    "    \n",
    "    # calculate roc auc metric\n",
    "    fpr, tpr, thresholds = roc_curve(test_y, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # calculate brier loss for probability accuracy\n",
    "    brier_loss = brier_score_loss(test_y, probs)\n",
    "            \n",
    "    print(\"ROC AUC: {}\\nBrier loss: {}\".format(np.round(roc_auc, 3), np.round(brier_loss, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running adanet on antenatal for _hie\n",
      "Linear classifier\n",
      "ROC AUC: 0.653\n",
      "Brier loss: 0.753\n",
      "NN\n",
      "ROC AUC: 0.658\n",
      "Brier loss: 0.291\n",
      "BT\n",
      "ROC AUC: 0.653\n",
      "Brier loss: 0.055\n",
      "ensemble\n"
     ]
    }
   ],
   "source": [
    "for data in [\"antenatal\", \"antenatal_growth\", \"antenatal_intrapartum\"]:    \n",
    "    for outcome in ['_hie', '_lapgar', '_perinataldeath', '_resus']:\n",
    "        \n",
    "        print(\"Running adanet on {} for {}\".format(data, outcome))\n",
    "        \n",
    "        # read in data\n",
    "        train = pd.read_csv(\"data/{}{}_train.csv\".format(data, outcome), index_col=0).astype('float32')\n",
    "        test = pd.read_csv(\"data/{}{}_test.csv\".format(data, outcome), index_col=0).astype('float32')\n",
    "        train_y = train.pop(outcome)\n",
    "        test_y = test.pop(outcome)\n",
    "        \n",
    "        # record feature columns\n",
    "        feature_columns = []\n",
    "        \n",
    "        for feature_name in train.columns:\n",
    "            feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
    "        \n",
    "        train_input_fn = make_input_fn(train, train_y)\n",
    "        eval_input_fn = make_input_fn(test, test_y, num_epochs=1, shuffle=False)\n",
    "\n",
    "        # linear classifier\n",
    "        print(\"Linear classifier\")\n",
    "        clf = tf.estimator.LinearClassifier(\n",
    "            feature_columns=feature_columns\n",
    "        )\n",
    "        clf.train(train_input_fn)\n",
    "        get_metrics(clf.predict(eval_input_fn))\n",
    "        \n",
    "        # NN\n",
    "        print(\"NN\")\n",
    "        clf = tf.estimator.DNNClassifier(\n",
    "            feature_columns=feature_columns, \n",
    "            hidden_units=[100, 20, 10]\n",
    "        )\n",
    "        clf.train(train_input_fn)\n",
    "        get_metrics(clf.predict(eval_input_fn))\n",
    "\n",
    "        # BT\n",
    "        print(\"BT\")\n",
    "        clf = tf.estimator.BoostedTreesClassifier(\n",
    "            feature_columns=feature_columns,\n",
    "            n_batches_per_layer=400\n",
    "        )\n",
    "        clf.train(train_input_fn)\n",
    "        get_metrics(clf.predict(eval_input_fn))\n",
    "        \n",
    "        # ensemble\n",
    "        print(\"ensemble\")\n",
    "        head = tf.estimator.BinaryClassHead()\n",
    "        clf = adanet.AutoEnsembleEstimator(\n",
    "            head=head,\n",
    "            candidate_pool=lambda config: {\n",
    "                \"linear\": tf.estimator.LinearClassifier(\n",
    "                    feature_columns=feature_columns,\n",
    "                    config=config\n",
    "                ),\n",
    "                \"nn_20\": tf.estimator.DNNClassifier(\n",
    "                    feature_columns=feature_columns,\n",
    "                    config=config,\n",
    "                    hidden_units=[20]\n",
    "                ),\n",
    "                \"nn_50\": tf.estimator.DNNClassifier(\n",
    "                    feature_columns=feature_columns,\n",
    "                    config=config,\n",
    "                    hidden_units=[50]\n",
    "                ),\n",
    "                \"nn_100\": tf.estimator.DNNClassifier(\n",
    "                    feature_columns=feature_columns,\n",
    "                    config=config,\n",
    "                    hidden_units=[100]\n",
    "                ),\n",
    "                \"nn_20_10\": tf.estimator.DNNClassifier(\n",
    "                    feature_columns=feature_columns,\n",
    "                    config=config,\n",
    "                    hidden_units=[20, 10]\n",
    "                ),\n",
    "                \"nn_50_20\": tf.estimator.DNNClassifier(\n",
    "                    feature_columns=feature_columns,\n",
    "                    config=config,\n",
    "                    hidden_units=[50, 20]\n",
    "                ),\n",
    "                \"nn_100_40\": tf.estimator.DNNClassifier(\n",
    "                    feature_columns=feature_columns,\n",
    "                    config=config,\n",
    "                    hidden_units=[100, 40]\n",
    "                ),\n",
    "                \"bt\": tf.estimator.BoostedTreesClassifier(\n",
    "                    feature_columns=feature_columns,\n",
    "                    config=config,\n",
    "                    n_batches_per_layer=400\n",
    "                )\n",
    "            },\n",
    "            max_iteration_steps=100\n",
    "        )\n",
    "\n",
    "        clf.train(train_input_fn)\n",
    "        get_metrics(clf.predict(eval_input_fn))\n",
    "        \n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
